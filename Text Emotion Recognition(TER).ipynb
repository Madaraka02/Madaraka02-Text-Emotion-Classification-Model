{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbebb5ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import Bidirectional\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Dropout\n",
    "\n",
    "\n",
    "import keras\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Dense, Flatten, LSTM\n",
    "from keras.layers import Input, GlobalMaxPool1D, Dropout\n",
    "from keras.layers import Activation\n",
    "from keras.models import Model, Sequential\n",
    "from keras import optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d454570d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "307ac100",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bad7b698",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('datasets/geo/data/full_dataset/goemotions_1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c78d7b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d451aefa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.read_csv('datasets/geo/data/full_dataset/goemotions_2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8799d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f46ade8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = pd.read_csv('datasets/geo/data/full_dataset/goemotions_3.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50edff19",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caa26b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#concatenate the 3 datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "944f6063",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([df1, df2, df3], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea1d9cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f450202",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "841da52e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Delete unwanted columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6927d982",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(['created_utc','id', 'author', 'subreddit', 'link_id', 'parent_id', \n",
    "         'rater_id', 'example_very_unclear'], \n",
    "        inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88116b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1ba7f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# available emotions\n",
    "emotions = []\n",
    "for col in df.columns:\n",
    "    emotions.append(col)\n",
    "    \n",
    "emotions_df = pd.DataFrame(emotions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b3688e",
   "metadata": {},
   "outputs": [],
   "source": [
    "emotions_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82553a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "pucn = string.punctuation\n",
    "stop = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a6b151c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_sentences(sentence):\n",
    "    url = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    url_free = url.sub(r'', sentence)\n",
    "    alpha = ''.join([word for word in url_free if not word.isdigit()])\n",
    "    no_white_space = re.sub(r'\\s+', ' ',  alpha)  \n",
    "    no_non_words = re.sub('\\w*\\d\\w*', '', no_white_space)\n",
    "    html=re.compile(r'<.*?>')\n",
    "    no_html = html.sub(r'',no_non_words)\n",
    "    no_punct=[words for words in no_html if words not in pucn]\n",
    "    words_wo_punct=''.join(no_punct)\n",
    "    words_wo_punct = words_wo_punct.lower()\n",
    "    emoji = re.compile(\"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\" \n",
    "        u\"\\U0001F300-\\U0001F5FF\"  \n",
    "        u\"\\U0001F680-\\U0001F6FF\"\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\" \n",
    "        u\"\\U00002500-\\U00002BEF\"  \n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U000024C2-\\U0001F251\"\n",
    "        u\"\\U0001f926-\\U0001f937\"\n",
    "        u\"\\U00010000-\\U0010ffff\"\n",
    "        u\"\\u2640-\\u2642\" \n",
    "        u\"\\u2600-\\u2B55\"\n",
    "        u\"\\u200d\"\n",
    "        u\"\\u23cf\"\n",
    "        u\"\\u23e9\"\n",
    "        u\"\\u231a\"\n",
    "        u\"\\ufe0f\"  # dingbats\n",
    "        u\"\\u3030\"\"]+\", re.UNICODE)\n",
    "    no_emoji = re.sub(emoji, '', words_wo_punct)\n",
    "    return no_emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e305bb5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "comments = list(df['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "848d96ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f040d22e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentence = comments[200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e05fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d55e8ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_text = clean_sentences(test_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b9faafc",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52dfe1b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X =[]\n",
    "for sentence  in comments:\n",
    "    X.append(clean_sentences(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be845a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X[200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c40d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "emotions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "479d9f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(emotions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e622e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df[[\n",
    " 'admiration',\n",
    " 'amusement',\n",
    " 'anger',\n",
    " 'annoyance',\n",
    " 'approval',\n",
    " 'caring',\n",
    " 'confusion',\n",
    " 'curiosity',\n",
    " 'desire',\n",
    " 'disappointment',\n",
    " 'disapproval',\n",
    " 'disgust',\n",
    " 'embarrassment',\n",
    " 'excitement',\n",
    " 'fear',\n",
    " 'gratitude',\n",
    " 'grief',\n",
    " 'joy',\n",
    " 'love',\n",
    " 'nervousness',\n",
    " 'optimism',\n",
    " 'pride',\n",
    " 'realization',\n",
    " 'relief',\n",
    " 'remorse',\n",
    " 'sadness',\n",
    " 'surprise',\n",
    " 'neutral']].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af03dc79",
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cec707e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test=train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55167bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer() \n",
    "tokenizer.fit_on_texts(X_train) # create word to index relationship-each word is given a unique index mostly according to appearance in the corpus\n",
    "\n",
    "vocab_size = len(tokenizer.word_index) + 1 #size of the total number of uniques tokens in the dataset\n",
    "\n",
    "tokenized_train = tokenizer.texts_to_sequences(X_train) #converting tokens into sequence of integers\n",
    "tokenized_test = tokenizer.texts_to_sequences(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27cb1f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3006b5a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_train[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b77a6a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 100\n",
    "\n",
    "padded_train = pad_sequences(tokenized_train, maxlen=max_len, padding='pre') \n",
    "\n",
    "padded_test = pad_sequences(tokenized_test, maxlen=max_len, padding='pre')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16b6b0ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_train[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "900874d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "y[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36d7b80c",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size #all unique tokens in the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63cb4229",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "create an embedding matrix with glove words  and vectors \n",
    "- each word is a key and a corresponding vector is the value\n",
    "\"\"\"\n",
    "emmbeddings_dictionary = {}\n",
    "\n",
    "glove_model =  open('glove.6B.100d.txt', encoding='utf8')\n",
    "    \n",
    "for line in glove_model:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    vector = np.asarray(values[1:],'float32')\n",
    "    emmbeddings_dictionary[word]=vector\n",
    "    \n",
    "glove_model.close()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e39694",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "create an embedding matrix for the corpus where each word in the corpus is a key and the corresponding vector \n",
    "of a dimension of 100 is the value\n",
    "each row contains a number which corresponds to the index of the word in the corpus\n",
    "the matrix has 100 columns-each column contains word embedding for each word in the corpus\n",
    "\"\"\"\n",
    "\n",
    "embedding_dim=100\n",
    "embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "\n",
    "for word, index in tokenizer.word_index.items():\n",
    "    embedding_vector = emmbeddings_dictionary.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[index]=embedding_vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b764304d",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "150a3be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential() #initialize sequential model\n",
    "\n",
    "# create embedding layer\n",
    "embedding_layer = Embedding(vocab_size, embedding_dim,\n",
    "                            weights=[embedding_matrix], input_length=max_len, trainable=False)\n",
    "\n",
    "# add layers to the model\n",
    "model.add(embedding_layer)\n",
    "model.add(LSTM(256, return_sequences = True))\n",
    "# model.add(Dropout(0.2))\n",
    "model.add(LSTM(128))\n",
    "# model.add(Dropout(0.2))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(28, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62e91305",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06a309b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d9cfeb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "history=model.fit(padded_train, y_train, epochs=200, batch_size=128, validation_split=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba0be2fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(padded_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70f228ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('EmoText')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
